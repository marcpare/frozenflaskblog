title: Panoramas, Python, & Pigeons
published: !!timestamp '2014-07-05 11:00:00'
active_page: blog
tags:
    - python
    - programming

What I thought would be a few minutes of poking on OpenCV to stitch some images together turned out to be a week-long dive into computer vision, linear algebra, geometry, eventually surfacing to meet our friend, the pigeon.

First, let's do a basic implementation of panorama stitching in Python. There's a great page by Dr. Christian Richardt on [practical exercises on Open CV](http://richardt.name/teaching/supervisions/vision-2011/practical/):

I did each of the exercises in item 3, "Panorama stitching". You can find [the full repo on Github](https://github.com/marcpare/stitch). The repo includes a Vagrantfile and a shell script so that you can get OpenCV running without much yak shaving.

If you just want to solve the stitching problem, you can actually ignore the tricky part of this, item (3). However, if you happen to want to stitch more than two images together, you need to have a robust offset and dimensions calculation.

This has some fiddly details, and it takes a bit of linear algebra. [The code](https://github.com/marcpare/stitch/blob/master/crichardt/stitch.py) is probably the clearest way to walk through this if you want to know the details. 

One tricky bit is that when you apply the transformation that lines up one image with the other, you end up with a combined image that is larger. To get the images placed properly on the bigger image, you need to offset them. 

![Offset](/static/images/20140705-offset-640.jpg)

For placing Image 2 properly, the offset has to be combined with the transformation (homography) matrix. For a translation (OX, OY), the transformation matrix is...

![Transform](/static/images/20140705-transform.jpg)

We combine it with the homography matrix via matrix multiplication:  

  homography = translation * homography
  
The resulting code should be pretty clear now:

<pre class="prettyprint">
translation = np.matrix([
  [1.0, 0.0, ox],
  [0.0, 1.0, oy],
  [0.0, 0.0, 1.0]
])

homography = translation * homography
</pre>

Nice!

***

Eventually, I get a panorama of the view outside my office stitched together:

![Pano](/static/images/20140705-pano1.jpg)

Then I stitch another:

![Pano](/static/images/20140705-pano2.jpg)

Now I'm getting confident. Let's try to make a panorama of my bookcase. I'm going to do something different this time. I'm going to walk right to left, snapping pictures as I go. I've done all this fancy linear algebra which will _surely_ take care of everything automatically

![Buggy books](/static/images/20140705-coolbug.jpg)

Uh oh. Maybe if I change this one thing...

![bug 2](/static/images/20140705-bug3.png)

Uh. Surely this change will fix it.

![bug 2](/static/images/20140705-bug2.jpg)

!!!

(no exaggerating, these are real bugs that I encountered.)

After many hours of head scratching, I realize that I have made a major conceptual mistake. It turns out there's a fundamental difference between the two images I was trying to create. One was a "panorama" while the other was a "mosaic". 

It's a panorama if you stand in one place and only rotate the camera.

It's a mosaic if you translate the camera.

This is important because you can stitch panorama photos fairly easily using the homography matrix. That's what we did above.

Unfortunately, for mosaics, this doesn't work. There isn't an affine transform that can relate two pictures in a mosaic.

The argument is as follows: as I move from left to right, I gradually see parts of books that I couldn't see before (this is parallax in action). How could you use an affine transformation to line these up? You can't without some knowledge of the 3D geometry of the subject, which we can't derive from just two pictures.

![Buggy books](/static/images/20140705-01.jpg)
![Buggy books](/static/images/20140705-02.jpg)

When you're only rotating the camera, this doesn't matter. An object that shows up in both pictures will look the same. You can verify this by rotating your head around your neck right now. Focus on an object anywhere in the room. As long as you're only rotating your head, the object will look exactly the same no matter what position your head happens to be in. 

Fascinating, right? There's something very different about moving around (translation) versus spinning around (rotation).

Consequently, there isn't a nice way to automate stitching mosaics together. In the posts by other professionals that do this, I found the advice: [take a bunch of pictures](http://dinosaurpalaeo.wordpress.com/2013/01/20/using-hugin-part-4-mosaic-images/).

But what if I don't necessarily want to stitch the images together? Here's a variation I came up with to visualize my book case.

* Use feature detection to find alignment points in the images. 
* Calculate a scale+translation matrix to align them. [2] 
* As you hover over the images, reveal the image nearest your cursor.

![ui prototype](/static/images/20140705-ss.png)
[http://smallredtile.com/align/](http://smallredtile.com/align/
)

***

The big takeaway from this exploration is that there are many knobs and dials in image stitching algorithms. There is surprising nuance to perception. Things that seems similar can actually be quite different in character. You can really shape and tune the algorithms to fit your application; it's not one-size-fits-all. 

It turns out that pigeons use the parallax effect that stymies image mosaics to their advantage: with non-overlapping fields of vision from the eyes on the sides of their heads, they bob their heads to get two different images at slightly different perspectives, allowing them to calculate depth.